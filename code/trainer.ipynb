{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Progbar\n",
    "import tensorflow as tf\n",
    "\n",
    "import math\n",
    "\n",
    "from models import SMD_Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, epochs, batch, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.batch = batch\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    # loss 함수 계산하는 부분 추가해야 됨\n",
    "    def compute_acc(self, y_pred, y):\n",
    "        correct = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        return accuracy\n",
    "\n",
    "    @tf.function\n",
    "    def train_on_batch(self, x_batch_train, y_batch_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.model(x_batch_train, training=True)    # 모델이 예측한 결과\n",
    "            train_loss = self.loss_fn(y_batch_train, logits)     # 모델이 예측한 결과와 GT를 이용한 loss 계산\n",
    "\n",
    "        grads = tape.gradient(train_loss, self.model.trainable_weights)  # gradient 계산\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))  # Otimizer에게 처리된 그라데이션 적용을 요청\n",
    "\n",
    "        return train_loss, logits\n",
    "\n",
    "    def train(self, train_dataset, acc_metric, steps_per_epoch, val_dataset, val_step):\n",
    "        metrics_names = ['train_loss', 'train_acc', 'val_loss']\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(\"\\nEpoch {}/{}\".format(epoch+1, self.epochs))\n",
    "\n",
    "            # train_dataset = train_dataset.take(steps_per_epoch)\n",
    "            # val_dataset = val_dataset.take(val_step)\n",
    "\n",
    "            progBar = Progbar(steps_per_epoch * self.batch, stateful_metrics=metrics_names)\n",
    "\n",
    "            train_loss, val_loss = 100, 100\n",
    "\n",
    "            # 데이터 집합의 배치에 대해 반복합니다\n",
    "            for step_train, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "                train_loss, logits = self.train_on_batch(x_batch_train, y_batch_train)\n",
    "\n",
    "                # train metric(mean, auc, accuracy 등) 업데이트\n",
    "                acc_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "                train_acc = self.compute_acc(logits, y_batch_train)\n",
    "                values = [('train_loss', train_loss), ('train_acc', train_acc)]\n",
    "                # print('{}'.format((step_train + 1) * self.batch))\n",
    "                progBar.update((step_train + 1) * self.batch, values=values)\n",
    "\n",
    "            for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):\n",
    "                logits = self.model(x_batch_val, training=False)\n",
    "                val_loss = self.loss_fn(y_batch_val, logits)\n",
    "                val_acc = self.compute_acc(logits, y_batch_val)\n",
    "                values = [('train_loss', train_loss), ('train_acc', train_acc), ('val_loss', val_loss), ('val_acc', val_acc)]\n",
    "            progBar.update((step_train + 1) * self.batch, values=values, finalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = '/aiffel/aiffel/model-fit/data/DATASET/TRAIN' \n",
    "val_data_path = '/aiffel/aiffel/model-fit/data/DATASET/VAL' \n",
    "batch_size = 16\n",
    "\n",
    "train_ds, TRAIN_SIZE = load_data(data_path=train_data_path, img_shape=(224, 224), batch_size=batch_size)\n",
    "val_ds, VAL_SIZE = load_data(data_path=val_data_path, img_shape=(224, 224), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.3)  \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001) \n",
    "acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "model = YogaPose(num_classes=5)\n",
    "\n",
    "# epoch을 조정해 보세요. \n",
    "trainer = Trainer(model=model,\n",
    "                  epochs=100,\n",
    "                  batch=1,\n",
    "                  loss_fn=loss_function,\n",
    "                  optimizer=optimizer,)\n",
    "\n",
    "trainer.train(train_dataset=train_ds,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            val_step=val_steps,\n",
    "            val_dataset=val_ds,\n",
    "            acc_metric=acc_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고 코드\n",
    "# https://pyimagesearch.com/2018/06/04/keras-multiple-outputs-and-multiple-losses/\n",
    "\n",
    "# initialize our FashionNet multi-output network\n",
    "model = FashionNet.build(96, 96,\n",
    "\tnumCategories=len(categoryLB.classes_),\n",
    "\tnumColors=len(colorLB.classes_),\n",
    "\tfinalAct=\"softmax\")\n",
    "# define two dictionaries: one that specifies the loss method for\n",
    "# each output of the network along with a second dictionary that\n",
    "# specifies the weight per loss\n",
    "losses = {\n",
    "\t\"category_output\": \"categorical_crossentropy\",\n",
    "\t\"color_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {\"category_output\": 1.0, \"color_output\": 1.0}\n",
    "# initialize the optimizer and compile the model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(optimizer=opt, loss=losses, loss_weights=lossWeights,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "H = model.fit(x=trainX,\n",
    "\ty={\"category_output\": trainCategoryY, \"color_output\": trainColorY},\n",
    "\tvalidation_data=(testX,\n",
    "\t\t{\"category_output\": testCategoryY, \"color_output\": testColorY}),\n",
    "\tepochs=EPOCHS,\n",
    "\tverbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
